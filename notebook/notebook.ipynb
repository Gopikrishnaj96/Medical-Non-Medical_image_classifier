{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2d9bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Upload your dataset.zip to Colab first\n",
    "zip_path = '/content/binary_medical_classifier_dataset.zip'\n",
    "\n",
    "# Extract to a known directory\n",
    "extract_dir = 'extracted_data'\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir)\n",
    "\n",
    "# NOW verify what you actually have\n",
    "print(\"Actual directory structure:\")\n",
    "for root, dirs, files in os.walk(extract_dir):\n",
    "    level = root.replace(extract_dir, '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f'{indent}{os.path.basename(root)}/')\n",
    "    subindent = ' ' * 2 * (level + 1)\n",
    "    for file in files[:3]:\n",
    "        print(f'{subindent}{file}')\n",
    "    if len(files) > 3:\n",
    "        print(f'{subindent}... and {len(files)-3} more files')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a683f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# ==================== STEP 1: FIX YOUR DATASET ====================\n",
    "def fix_dataset_completely(input_dir, output_dir, target_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Fix all images in dataset - resize, convert to RGB, save properly\n",
    "    \"\"\"\n",
    "    print(\"üîß FIXING DATASET - STANDARDIZING ALL IMAGES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    fixed_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    for root, dirs, files in os.walk(input_dir):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff', '.webp')):\n",
    "                input_path = os.path.join(root, file)\n",
    "                \n",
    "                # Create output directory structure\n",
    "                rel_path = os.path.relpath(root, input_dir)\n",
    "                output_folder = os.path.join(output_dir, rel_path)\n",
    "                os.makedirs(output_folder, exist_ok=True)\n",
    "                output_path = os.path.join(output_folder, file)\n",
    "                \n",
    "                try:\n",
    "                    with Image.open(input_path) as img:\n",
    "                        # Convert to RGB\n",
    "                        if img.mode != 'RGB':\n",
    "                            if img.mode == 'L':  # Grayscale\n",
    "                                img = img.convert('RGB')\n",
    "                            elif img.mode == 'RGBA':  # Has alpha\n",
    "                                background = Image.new('RGB', img.size, (255, 255, 255))\n",
    "                                background.paste(img, mask=img.split()[3])\n",
    "                                img = background\n",
    "                            else:\n",
    "                                img = img.convert('RGB')\n",
    "                        \n",
    "                        # Resize to target size\n",
    "                        if img.size != target_size:\n",
    "                            img = img.resize(target_size, Image.Resampling.LANCZOS)\n",
    "                        \n",
    "                        # Save as high quality JPEG\n",
    "                        img.save(output_path, 'JPEG', quality=95)\n",
    "                        fixed_count += 1\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Failed to process {input_path}: {e}\")\n",
    "                    error_count += 1\n",
    "    \n",
    "    print(f\"\\n‚úÖ DATASET FIX COMPLETE:\")\n",
    "    print(f\"   Fixed images: {fixed_count:,}\")\n",
    "    print(f\"   Errors: {error_count}\")\n",
    "    print(f\"   All images now: {target_size}\")\n",
    "    \n",
    "    return fixed_count, error_count\n",
    "\n",
    "# ==================== STEP 2: VERIFY DATASET ====================\n",
    "def verify_fixed_dataset(dataset_dir):\n",
    "    \"\"\"\n",
    "    Verify all images are properly formatted\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîç VERIFYING FIXED DATASET: {dataset_dir}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    total_images = 0\n",
    "    size_distribution = Counter()\n",
    "    mode_distribution = Counter()\n",
    "    errors = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(dataset_dir):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                filepath = os.path.join(root, file)\n",
    "                try:\n",
    "                    with Image.open(filepath) as img:\n",
    "                        size_distribution[img.size] += 1\n",
    "                        mode_distribution[img.mode] += 1\n",
    "                        total_images += 1\n",
    "                except Exception as e:\n",
    "                    errors.append(f\"{filepath}: {e}\")\n",
    "    \n",
    "    print(f\"üìä VERIFICATION RESULTS:\")\n",
    "    print(f\"   Total images: {total_images:,}\")\n",
    "    print(f\"   Unique sizes: {len(size_distribution)}\")\n",
    "    print(f\"   Errors: {len(errors)}\")\n",
    "    \n",
    "    # Show size distribution\n",
    "    for size, count in size_distribution.most_common():\n",
    "        status = \"‚úÖ\" if size == (224, 224) else \"‚ùå\"\n",
    "        percentage = (count / total_images * 100) if total_images > 0 else 0\n",
    "        print(f\"   {status} Size {size}: {count:,} images ({percentage:.1f}%)\")\n",
    "    \n",
    "    all_correct_size = len(size_distribution) == 1 and (224, 224) in size_distribution\n",
    "    all_rgb = len(mode_distribution) == 1 and 'RGB' in mode_distribution\n",
    "    \n",
    "    if all_correct_size and all_rgb and len(errors) == 0:\n",
    "        print(\"‚úÖ DATASET IS PROPERLY FORMATTED\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"‚ùå DATASET STILL HAS ISSUES\")\n",
    "        return False\n",
    "\n",
    "# ==================== STEP 3: WORKING TRAINING PIPELINE ====================\n",
    "def create_simple_dataset(data_dir, batch_size=32, validation_split=0.2):\n",
    "    \"\"\"\n",
    "    Create dataset using TensorFlow's built-in functions - FIXED VERSION\n",
    "    \"\"\"\n",
    "    # Get the original dataset with class_names BEFORE transformations\n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        data_dir,\n",
    "        validation_split=validation_split,\n",
    "        subset=\"training\",\n",
    "        seed=123,\n",
    "        image_size=(224, 224),\n",
    "        batch_size=batch_size,\n",
    "        label_mode='binary'\n",
    "    )\n",
    "    \n",
    "    val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        data_dir,\n",
    "        validation_split=validation_split,\n",
    "        subset=\"validation\",\n",
    "        seed=123,\n",
    "        image_size=(224, 224),\n",
    "        batch_size=batch_size,\n",
    "        label_mode='binary'\n",
    "    )\n",
    "    \n",
    "    # CRITICAL: Get class_names BEFORE transformations remove this attribute\n",
    "    class_names = train_ds.class_names\n",
    "    \n",
    "    # Now apply transformations - FIXED: Use tf.keras.layers.Rescaling, not tf.keras.utils.Rescaling\n",
    "    normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "    train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "    val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "    \n",
    "    # Performance optimization\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    \n",
    "    return train_ds, val_ds, class_names\n",
    "\n",
    "def create_simple_model():\n",
    "    \"\"\"\n",
    "    Simple CNN model appropriately sized for your dataset\n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "        # Data augmentation\n",
    "        tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "        tf.keras.layers.RandomRotation(0.1),\n",
    "        tf.keras.layers.RandomZoom(0.1),\n",
    "        \n",
    "        # CNN layers\n",
    "        tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Conv2D(128, 3, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        \n",
    "        # Classification head\n",
    "        tf.keras.layers.GlobalAveragePooling2D(),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_medical_classifier(dataset_path, epochs=20, batch_size=32):\n",
    "    \"\"\"\n",
    "    Complete training pipeline that actually works - FIXED VERSION\n",
    "    \"\"\"\n",
    "    print(\"üè• MEDICAL IMAGE CLASSIFIER TRAINING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create datasets with FIXED class_names handling\n",
    "    train_ds, val_ds, class_names = create_simple_dataset(\n",
    "        f'{dataset_path}/train', \n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    # Handle test dataset if exists\n",
    "    test_path = f'{dataset_path}/test'\n",
    "    if os.path.exists(test_path):\n",
    "        test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "            test_path,\n",
    "            image_size=(224, 224),\n",
    "            batch_size=batch_size,\n",
    "            label_mode='binary'\n",
    "        )\n",
    "        normalization = tf.keras.layers.Rescaling(1./255)\n",
    "        test_ds = test_ds.map(lambda x, y: (normalization(x), y))\n",
    "        test_ds = test_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
    "    else:\n",
    "        test_ds = None\n",
    "    \n",
    "    # Now class_names works properly - FIXED\n",
    "    print(f\"üìä Classes found: {class_names}\")\n",
    "    \n",
    "    # Create and compile model\n",
    "    model = create_simple_model()\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(\"\\nüèóÔ∏è MODEL ARCHITECTURE:\")\n",
    "    model.summary()\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.2,\n",
    "            patience=3\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Train model\n",
    "    print(f\"\\nüöÄ Starting training for {epochs} epochs...\")\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    \n",
    "    # Evaluate on test set if available\n",
    "    if test_ds is not None:\n",
    "        print(\"\\nüß™ EVALUATING ON TEST SET:\")\n",
    "        test_loss, test_accuracy = model.evaluate(test_ds)\n",
    "        print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save model\n",
    "    model.save('medical_classifier.keras')\n",
    "    print(\"\\nüíæ Model saved as 'medical_classifier.keras'\")\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# ==================== STEP 4: TEST SINGLE IMAGES ====================\n",
    "def test_single_image(model_path, image_path, class_names=['medical', 'non_medical']):\n",
    "    \"\"\"\n",
    "    Test the trained model on a single image\n",
    "    \"\"\"\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    \n",
    "    # Load and preprocess image\n",
    "    img = tf.keras.utils.load_img(image_path, target_size=(224, 224))\n",
    "    img_array = tf.keras.utils.img_to_array(img)\n",
    "    img_array = tf.expand_dims(img_array, 0)  # Create batch dimension\n",
    "    img_array = img_array / 255.0  # Normalize\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(img_array)[0][0]\n",
    "    predicted_class = class_names[int(prediction > 0.5)]\n",
    "    confidence = prediction if prediction > 0.5 else (1 - prediction)\n",
    "    \n",
    "    print(f\"Image: {image_path}\")\n",
    "    print(f\"Prediction: {predicted_class}\")\n",
    "    print(f\"Confidence: {confidence:.3f}\")\n",
    "    \n",
    "    return predicted_class, confidence\n",
    "\n",
    "# ==================== MAIN EXECUTION ====================\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Complete pipeline from broken dataset to trained model\n",
    "    \"\"\"\n",
    "    original_dataset = \"binary_medical_classifier\"\n",
    "    fixed_dataset = \"binary_medical_classifier_fixed\"\n",
    "    \n",
    "    print(\"üöÄ COMPLETE MEDICAL IMAGE CLASSIFIER PIPELINE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Step 1: Fix the dataset\n",
    "    if not os.path.exists(fixed_dataset):\n",
    "        print(\"Step 1: Fixing dataset...\")\n",
    "        fixed_count, error_count = fix_dataset_completely(original_dataset, fixed_dataset)\n",
    "        \n",
    "        if fixed_count == 0:\n",
    "            print(\"‚ùå No images found or processed. Check your dataset path.\")\n",
    "            return\n",
    "    else:\n",
    "        print(f\"Step 1: Using existing fixed dataset at {fixed_dataset}\")\n",
    "    \n",
    "    # Step 2: Verify the dataset\n",
    "    print(\"\\nStep 2: Verifying dataset...\")\n",
    "    is_valid = verify_fixed_dataset(fixed_dataset)\n",
    "    \n",
    "    if not is_valid:\n",
    "        print(\"‚ùå Dataset verification failed. Check the issues above.\")\n",
    "        return\n",
    "    \n",
    "    # Step 3: Train the model\n",
    "    print(\"\\nStep 3: Training model...\")\n",
    "    model, history = train_medical_classifier(fixed_dataset, epochs=15, batch_size=32)\n",
    "    \n",
    "    print(\"\\n‚úÖ PIPELINE COMPLETE!\")\n",
    "    print(\"Your medical image classifier is ready to use.\")\n",
    "\n",
    "# ==================== EXECUTE EVERYTHING ====================\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    \n",
    "    # Optional: Test on a single image after training\n",
    "    # test_single_image('medical_classifier.keras', 'path/to/test/image.jpg')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
